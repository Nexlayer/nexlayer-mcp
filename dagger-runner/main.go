package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"strconv"
	"strings"

	"dagger.io/dagger"
	"github.com/google/uuid"
)

// BuildResult represents the output of the build process
type BuildResult struct {
	Client      string         `json:"client,omitempty"`
	Server      string         `json:"server,omitempty"`
	Ports       map[string]int `json:"ports"`
	Error       string         `json:"error,omitempty"`
	DAGSummary  string         `json:"dagSummary,omitempty"`
	LLMInsights string         `json:"llmInsights,omitempty"`
}

// ServiceConfig holds configuration for a service to be built
type ServiceConfig struct {
	Name             string
	Path             string
	Port             int
	DockerfileExists bool
}

func main() {
	if len(os.Args) < 2 || len(os.Args) > 4 {
		result := BuildResult{
			Error: "Usage: nexlayer-dagger-runner <repo-path> [--llm-optimize] [--llm-provider=openai]",
		}
		outputResult(result)
		os.Exit(1)
	}

	repoPath := os.Args[1]
	llmOptimize := false
	llmProvider := "openai"

	// Parse optional arguments
	for i := 2; i < len(os.Args); i++ {
		arg := os.Args[i]
		if arg == "--llm-optimize" {
			llmOptimize = true
		} else if strings.HasPrefix(arg, "--llm-provider=") {
			llmProvider = strings.Split(arg, "=")[1]
		}
	}

	// Validate repo path exists
	if _, err := os.Stat(repoPath); os.IsNotExist(err) {
		result := BuildResult{
			Error: fmt.Sprintf("Repository path does not exist: %s", repoPath),
		}
		outputResult(result)
		os.Exit(1)
	}

	ctx := context.Background()

	// Initialize Dagger client
	client, err := dagger.Connect(ctx, dagger.WithLogOutput(os.Stderr))
	if err != nil {
		result := BuildResult{
			Error: fmt.Sprintf("Failed to connect to Dagger: %v", err),
		}
		outputResult(result)
		os.Exit(1)
	}
	defer client.Close()

	log.Printf("üîç Analyzing repository structure at: %s", repoPath)

	// Analyze repository structure
	services := analyzeRepository(repoPath)

	if len(services) == 0 {
		result := BuildResult{
			Error: "No buildable services found (client/ or server/ directories)",
			Ports: make(map[string]int),
		}
		outputResult(result)
		os.Exit(0)
	}

	log.Printf("üì¶ Found %d services to build", len(services))

	// Generate DAG summary for LLM analysis
	dagSummary := generateDAGSummary(services)

	// LLM-aware DAG optimization (if enabled)
	var llmInsights string
	if llmOptimize {
		llmInsights = performLLMDAGAnalysis(ctx, repoPath, services, llmProvider)
	}

	// Build and push images
	result := buildAndPushImages(ctx, client, repoPath, services)
	result.DAGSummary = dagSummary
	result.LLMInsights = llmInsights

	outputResult(result)

	if result.Error != "" {
		os.Exit(1)
	}
}

// analyzeRepository scans the repo for buildable services
func analyzeRepository(repoPath string) []ServiceConfig {
	var services []ServiceConfig

	// Check for client directory
	clientPath := filepath.Join(repoPath, "client")
	if stat, err := os.Stat(clientPath); err == nil && stat.IsDir() {
		dockerfileExists := checkDockerfileExists(clientPath)
		port := 3000 // Default React/frontend port

		// Try to parse port from Dockerfile if it exists
		if dockerfileExists {
			if parsedPort, err := parseDockerfilePort(clientPath); err == nil {
				port = parsedPort
				log.Printf("üìÑ Extracted port %d from client Dockerfile", port)
			} else {
				log.Printf("‚ö†Ô∏è  Could not parse port from client Dockerfile: %v, using default port %d", err, port)
			}
		}

		services = append(services, ServiceConfig{
			Name:             "client",
			Path:             clientPath,
			Port:             port,
			DockerfileExists: dockerfileExists,
		})
		log.Printf("‚úÖ Found client service at: %s (Dockerfile: %v, Port: %d)", clientPath, dockerfileExists, port)
	}

	// Check for server directory
	serverPath := filepath.Join(repoPath, "server")
	if stat, err := os.Stat(serverPath); err == nil && stat.IsDir() {
		dockerfileExists := checkDockerfileExists(serverPath)
		port := 5000 // Default backend port

		// Try to parse port from Dockerfile if it exists
		if dockerfileExists {
			if parsedPort, err := parseDockerfilePort(serverPath); err == nil {
				port = parsedPort
				log.Printf("üìÑ Extracted port %d from server Dockerfile", port)
			} else {
				log.Printf("‚ö†Ô∏è  Could not parse port from server Dockerfile: %v, using default port %d", err, port)
			}
		}

		services = append(services, ServiceConfig{
			Name:             "server",
			Path:             serverPath,
			Port:             port,
			DockerfileExists: dockerfileExists,
		})
		log.Printf("‚úÖ Found server service at: %s (Dockerfile: %v, Port: %d)", serverPath, dockerfileExists, port)
	}

	// Check for root-level service (fallback)
	if len(services) == 0 {
		if hasNodeProject(repoPath) {
			dockerfileExists := checkDockerfileExists(repoPath)
			port := 3000 // Default port

			// Try to parse port from Dockerfile if it exists
			if dockerfileExists {
				if parsedPort, err := parseDockerfilePort(repoPath); err == nil {
					port = parsedPort
					log.Printf("üìÑ Extracted port %d from root Dockerfile", port)
				} else {
					log.Printf("‚ö†Ô∏è  Could not parse port from root Dockerfile: %v, using default port %d", err, port)
				}
			}

			services = append(services, ServiceConfig{
				Name:             "app",
				Path:             repoPath,
				Port:             port,
				DockerfileExists: dockerfileExists,
			})
			if dockerfileExists {
				log.Printf("‚úÖ Found root-level project with MCP-generated Dockerfile (Port: %d)", port)
			} else {
				log.Printf("‚ö†Ô∏è  Found root-level project but no Dockerfile - needs nexlayer_generate_dockerfile")
			}
		}
	}

	return services
}

// checkDockerfileExists checks if a Dockerfile exists in the given directory
func checkDockerfileExists(dir string) bool {
	dockerfilePath := filepath.Join(dir, "Dockerfile")
	_, err := os.Stat(dockerfilePath)
	return err == nil
}

// parseDockerfilePort extracts the exposed port from a Dockerfile
func parseDockerfilePort(dir string) (int, error) {
	dockerfilePath := filepath.Join(dir, "Dockerfile")
	content, err := os.ReadFile(dockerfilePath)
	if err != nil {
		return 0, fmt.Errorf("failed to read Dockerfile: %v", err)
	}

	lines := strings.Split(string(content), "\n")
	for _, line := range lines {
		// Trim whitespace and convert to uppercase for case-insensitive matching
		trimmedLine := strings.TrimSpace(strings.ToUpper(line))

		// Look for EXPOSE directive
		if strings.HasPrefix(trimmedLine, "EXPOSE") {
			// Extract port number from EXPOSE directive
			parts := strings.Fields(trimmedLine)
			if len(parts) >= 2 {
				portStr := parts[1]
				// Handle cases like "EXPOSE 3000" or "EXPOSE 3000/tcp"
				if strings.Contains(portStr, "/") {
					portStr = strings.Split(portStr, "/")[0]
				}

				port, err := strconv.Atoi(portStr)
				if err != nil {
					return 0, fmt.Errorf("invalid port number in EXPOSE directive: %s", portStr)
				}

				if port < 1 || port > 65535 {
					return 0, fmt.Errorf("port number out of range: %d", port)
				}

				return port, nil
			}
		}
	}

	return 0, fmt.Errorf("no EXPOSE directive found in Dockerfile")
}

// hasNodeProject checks if a directory contains a Node.js project
func hasNodeProject(dir string) bool {
	packageJSONPath := filepath.Join(dir, "package.json")
	_, err := os.Stat(packageJSONPath)
	return err == nil
}

// buildAndPushImages builds and pushes container images for all services
func buildAndPushImages(ctx context.Context, client *dagger.Client, repoPath string, services []ServiceConfig) BuildResult {
	result := BuildResult{
		Ports: make(map[string]int),
	}

	// Generate unique identifier for this build
	buildID := strings.ToLower(uuid.New().String()[:8])

	for _, service := range services {
		log.Printf("üî® Building %s service...", service.Name)

		// Set up the container
		var container *dagger.Container
		var err error

		if service.DockerfileExists {
			// Use existing Dockerfile (generated by MCP)
			log.Printf("üìÑ Using Dockerfile for %s (generated by MCP)", service.Name)
			container, err = buildWithDockerfile(ctx, client, service.Path)
		} else {
			// Dockerfile should be generated by MCP first
			log.Printf("‚ùå No Dockerfile found for %s - MCP should generate Dockerfile first", service.Name)
			result.Error = fmt.Sprintf("No Dockerfile found for %s. Please use nexlayer_generate_dockerfile first to create optimized Dockerfile", service.Name)
			return result
		}

		if err != nil {
			result.Error = fmt.Sprintf("Failed to build %s: %v", service.Name, err)
			return result
		}

		// Generate image URL for ttl.sh
		imageURL := fmt.Sprintf("ttl.sh/%s-%s:1h", service.Name, buildID)
		log.Printf("üì§ Pushing %s to %s", service.Name, imageURL)

		// Push to registry
		publishedURL, err := container.Publish(ctx, imageURL)
		if err != nil {
			result.Error = fmt.Sprintf("Failed to push %s: %v", service.Name, err)
			return result
		}

		log.Printf("‚úÖ Successfully pushed %s: %s", service.Name, publishedURL)

		// Store results
		result.Ports[service.Name] = service.Port
		switch service.Name {
		case "client":
			result.Client = publishedURL
		case "server":
			result.Server = publishedURL
		default:
			// For single-service apps, put in client field
			if result.Client == "" {
				result.Client = publishedURL
			}
		}
	}

	return result
}

// buildWithDockerfile builds using an existing Dockerfile
func buildWithDockerfile(ctx context.Context, client *dagger.Client, servicePath string) (*dagger.Container, error) {
	// Mount the service directory
	sourceDir := client.Host().Directory(servicePath)

	// Build using the existing Dockerfile with linux/amd64 platform
	container := client.Container(dagger.ContainerOpts{
		Platform: dagger.Platform("linux/amd64"), // Force linux/amd64 for Nexlayer compatibility
	}).Build(sourceDir)

	return container, nil
}

// outputResult outputs the final result as JSON to stdout
func outputResult(result BuildResult) {
	jsonBytes, err := json.MarshalIndent(result, "", "  ")
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error marshaling result: %v\n", err)
		fmt.Println(`{"error": "Failed to marshal result"}`)
		return
	}

	fmt.Println(string(jsonBytes))
}

// Generate DAG summary for LLM analysis
func generateDAGSummary(services []ServiceConfig) string {
	summary := fmt.Sprintf("Build DAG for %d service(s):\n", len(services))

	for _, service := range services {
		if service.DockerfileExists {
			summary += fmt.Sprintf("- %s: Node.js service (port %d) [Dockerfile with EXPOSE %d]\n", service.Name, service.Port, service.Port)
		} else {
			summary += fmt.Sprintf("- %s: Node.js service (port %d) [Auto-generated Dockerfile]\n", service.Name, service.Port)
		}
	}

	summary += "\nBuild steps per service:\n"
	summary += "1. Parse Dockerfile EXPOSE directive for port detection\n"
	summary += "2. Platform: linux/amd64 (Nexlayer compatibility)\n"
	summary += "3. Use MCP-generated Dockerfile (nexlayer_generate_dockerfile)\n"
	summary += "4. Build container from Dockerfile\n"
	summary += "5. Push to ttl.sh registry\n"
	summary += "6. Return image URLs with detected ports for YAML generation\n"

	return summary
}

// Perform LLM DAG analysis and optimization
func performLLMDAGAnalysis(ctx context.Context, repoPath string, services []ServiceConfig, provider string) string {
	log.Printf("ü§ñ Starting LLM DAG analysis...")

	// Construct prompt for LLM
	prompt := buildLLMPrompt(repoPath, services)

	// Call LLM API (mock implementation - replace with actual LLM integration)
	response := mockLLMCall(prompt, provider)

	log.Printf("ü§ñ LLM analysis completed")
	return response
}

// Build comprehensive prompt for LLM analysis
func buildLLMPrompt(repoPath string, services []ServiceConfig) string {
	prompt := "You are an expert DevOps engineer analyzing a Dagger build pipeline. "
	prompt += "Based on the repository structure, suggest optimizations for the build process.\n\n"

	prompt += fmt.Sprintf("Repository: %s\n", repoPath)
	prompt += fmt.Sprintf("Services found: %d\n\n", len(services))

	for _, service := range services {
		prompt += fmt.Sprintf("Service: %s\n", service.Name)
		prompt += fmt.Sprintf("- Type: Node.js application\n")
		prompt += fmt.Sprintf("- Port: %d\n", service.Port)
		prompt += fmt.Sprintf("- Custom Dockerfile: %v\n", service.DockerfileExists)
		prompt += "\n"
	}

	prompt += "Current build workflow:\n"
	prompt += "1. MCP generates optimized Dockerfile via nexlayer_generate_dockerfile\n"
	prompt += "2. Dagger builds from MCP-generated Dockerfile (linux/amd64)\n"
	prompt += "3. Push built image to ttl.sh registry\n"
	prompt += "4. Return image URLs for nexlayer.yaml generation\n\n"

	prompt += "Please suggest optimizations for:\n"
	prompt += "1. MCP Dockerfile generation improvements\n"
	prompt += "2. Dagger build process enhancements\n"
	prompt += "3. ttl.sh registry optimization\n"
	prompt += "4. Nexlayer platform compatibility\n"
	prompt += "5. Build performance and security\n\n"

	prompt += "Focus on optimizations that work within the MCP ‚Üí Dagger ‚Üí Nexlayer workflow."

	return prompt
}

// Mock LLM call (replace with actual LLM integration)
func mockLLMCall(prompt string, provider string) string {
	log.Printf("ü§ñ Calling %s LLM API...", provider)

	// This is a mock response - in production, this would call OpenAI, Anthropic, etc.
	// Example of how to integrate:
	// if provider == "openai" {
	//     return callOpenAI(prompt)
	// } else if provider == "anthropic" {
	//     return callAnthropic(prompt)
	// }

	// Mock intelligent response based on new MCP ‚Üí Dagger ‚Üí Nexlayer workflow
	insights := "üß† LLM Analysis Results:\n\n"
	insights += "1. **Workflow**: ‚úÖ Using MCP-generated Dockerfiles (no local image building)\n"
	insights += "2. **Platform**: ‚úÖ Enforcing linux/amd64 for Nexlayer compatibility\n"
	insights += "3. **Registry**: ‚úÖ Using ttl.sh for temporary image hosting\n"
	insights += "4. **MCP Integration**: Consider caching Dockerfile generations for similar projects\n"
	insights += "5. **Build Optimization**: Multi-stage Dockerfiles reduce final image size\n"
	insights += "6. **Security**: MCP-generated Dockerfiles should include non-root users\n"
	insights += "7. **YAML Patching**: Image URLs properly integrated into nexlayer.yaml pods\n\n"
	insights += "üí° This workflow ensures consistent, optimized builds that work seamlessly with Nexlayer"

	return insights
}

// Optional: Real LLM integration examples (commented out for reference)
/*
func callOpenAI(prompt string) string {
	// Example OpenAI integration
	apiKey := os.Getenv("OPENAI_API_KEY")
	if apiKey == "" {
		return "OpenAI API key not configured"
	}

	// Make API call to OpenAI
	// ... implementation
	return "OpenAI response"
}

func callAnthropic(prompt string) string {
	// Example Anthropic integration
	apiKey := os.Getenv("ANTHROPIC_API_KEY")
	if apiKey == "" {
		return "Anthropic API key not configured"
	}

	// Make API call to Anthropic
	// ... implementation
	return "Anthropic response"
}
*/
